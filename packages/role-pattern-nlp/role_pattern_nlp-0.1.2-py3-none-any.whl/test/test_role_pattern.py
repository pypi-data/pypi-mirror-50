from pprint import pprint
import pytest
import json
import en_core_web_sm
from role_pattern_nlp import RolePatternBuilder, RolePatternSet
from role_pattern_nlp.exceptions import FeaturesNotInFeatureDictError
from role_pattern_nlp import util
import visualise_spacy_tree


idxs_to_tokens = util.idxs_to_tokens
nlp = en_core_web_sm.load()

text1 = 'We introduce efficient methods for fitting Boolean models to molecular data, successfully demonstrating their application to synthetic time courses generated by a number of established clock models, as well as experimental expression levels measured using luciferase imaging.'

text2 = 'The amyloid-beta oligomer hypothesis was introduced in 1998.'

text3 = 'L-theanine alone improved self-reported relaxation, tension, and calmness starting at 200 mg.'

doc1 = nlp(text1)
doc2 = nlp(text2)
doc3 = nlp(text3)

docs = [doc1, doc2, doc3]


cases = [
    {
        'example': {
            'doc': doc1,
            'match': {
                'slot1': idxs_to_tokens(doc1, [0]),  # [We]
                'slot2': idxs_to_tokens(doc1, [1]),  # [introduce]
                'slot3': idxs_to_tokens(doc1, [3]),  # [methods]
            },
        },
    },
    {
        'example': {
            'doc': doc1,
            'match': {
                'slot1': idxs_to_tokens(doc1, [13, 15]),  # [demonstrating, application]
                'slot2': idxs_to_tokens(doc1, [16, 19])  # [to, courses]
            },
        },
    },
    {
        'example': {
            'doc': doc1,
            'match': {
                'arg1': idxs_to_tokens(doc1, [19]),  # [courses]
                'pred': idxs_to_tokens(doc1, [20, 21]),  # [generated, by]
                'arg2': idxs_to_tokens(doc1, [27])  # [models]
            },
        },
    },
    {
        'example': {
            'doc': doc3,
            'match': {
                'ant': idxs_to_tokens(doc3, [2]),  # theanine
                'cons': idxs_to_tokens(doc3, [8]),  # relaxation
            }
        },
    }
]


feature_combs = [
    ['DEP', 'TAG', 'LOWER'],
    ['DEP', 'TAG'],
    ['DEP'],
]


def test_build_pattern_and_find_matches():
    feature_dict = {'DEP': 'dep_', 'TAG': 'tag_', 'LOWER': 'lower_'}
    for case in cases:
        doc = case['example']['doc']
        match_example = case['example']['match']
        role_pattern_builder = RolePatternBuilder(feature_dict)
        for features in feature_combs:
            role_pattern = role_pattern_builder.build(
                match_example, features=features, validate_pattern=True)
            matches = role_pattern.match(doc)
            assert match_example in matches, 'does not match example'
            print('passed')


def test_refine_pattern():
    match_example = {
        'arg1': idxs_to_tokens(doc1, [3]),  # [methods]
        'prep': idxs_to_tokens(doc1, [4]),  # [for]
        'arg2': idxs_to_tokens(doc1, [7]),  # [models]
    }
    neg_examples = [{
        'arg1': idxs_to_tokens(doc1, [3]),  # [methods]
        'prep': idxs_to_tokens(doc1, [8]),  # [to]
        'arg2': idxs_to_tokens(doc1, [10]),  # [data]
    }]
    # pprint(match_example)
    feature_dict = {'DEP': 'dep_', 'TAG': 'tag_', 'LOWER': 'lower_'}
    role_pattern_builder = RolePatternBuilder(feature_dict)
    pattern = role_pattern_builder.build(
        match_example, features=['DEP']
    )
    matches = pattern.match(doc1)
    assert match_example in matches
    assert neg_examples[0] in matches
    # pattern = role_pattern_builder.refine(doc1, pattern, match_example, neg_examples)
    refined_role_pattern_variants = role_pattern_builder.refine(pattern, match_example, neg_examples)
    for role_pattern_variant in refined_role_pattern_variants:
        matches = role_pattern_variant.match(doc1)
        assert match_example in matches
        assert neg_examples[0] not in matches


def test_validate_features():
    match_examples = [
        {
            'slot1': idxs_to_tokens(doc1, [0, 1, 3])  # [We, introduce, methods]
        },
    ]
    feature_dict = {'DEP': 'dep_', 'TAG': 'tag_'}
    role_pattern_builder = RolePatternBuilder(feature_dict)
    features = ['DEP', 'TAG', 'LOWER']
    for match_example in match_examples:
        with pytest.raises(FeaturesNotInFeatureDictError):
            role_pattern_builder.build(match_example, features=features)


def test_visualise_pattern():
    for i, doc in enumerate(docs):
        png = visualise_spacy_tree.create_png(doc)
        filepath = 'examples/sentence_vis/sentence_{}.png'.format(i)
        with open(filepath, 'wb') as f:
            f.write(png)
    feature_dict = {'DEP': 'dep_', 'TAG': 'tag_', 'LOWER': 'lower_'}
    for test_i, case in enumerate(cases):
        match_example = case['example']['match']
        role_pattern_builder = RolePatternBuilder(feature_dict)
        for features_i, features in enumerate(feature_combs):
            role_pattern = role_pattern_builder.build(match_example, features=features)
            filepath = 'examples/spacy_dep_patterns/pattern_{}_{}.json'.format(test_i, features_i)
            with open(filepath, 'w') as f:
                json.dump(role_pattern.spacy_dep_pattern, f, indent=2)
            pydot, legend = role_pattern.to_pydot(legend=True)
            png = pydot.create_png()
            filename = 'examples/pattern_vis/pattern_{0}_{1}.png'.format(test_i, features_i)
            with open(filename, 'wb') as f:
                f.write(png)


def test_visualise_pattern_legend():
    feature_dict = {'DEP': 'dep_', 'TAG': 'tag_', 'LOWER': 'lower_'}
    role_pattern_builder = RolePatternBuilder(feature_dict)
    case = cases[1]
    match_example = case['example']['match']
    role_pattern = role_pattern_builder.build(match_example)
    pydot, legend = role_pattern.to_pydot(legend=True)
    png = legend.create_png()
    filename = 'examples/pattern_vis/pattern_1_0_legend.png'
    with open(filename, 'wb') as f:
        f.write(png)


def test_visualise_pattern_match():
    feature_dict = {'DEP': 'dep_', 'TAG': 'tag_', 'LOWER': 'lower_'}
    for test_i, case in enumerate(cases):
        doc = case['example']['doc']
        match_example = case['example']['match']
        role_pattern_builder = RolePatternBuilder(feature_dict)
        for features_i, features in enumerate(feature_combs):
            role_pattern = role_pattern_builder.build(match_example, features=features)
            matches = role_pattern.match(doc)
            for match in matches:
                graph, legend = match.to_pydot(legend=True)
                png = graph.create_png()
                filename = 'examples/match_vis/match_{0}_{1}.png'.format(test_i, features_i)
                with open(filename, 'wb') as f:
                    f.write(png)
    png = legend.create_png()
    filename = 'examples/match_vis/match_{0}_{1}_legend.png'.format(test_i, features_i)
    with open(filename, 'wb') as f:
        f.write(png)


# def test_role_pattern_set():
#     match_examples = [
#         {
#             'slot1': idxs_to_tokens(doc1, [0, 1, 3]),  # [We, introduce, methods]
#         },
#         {
#             'slot1': idxs_to_tokens(doc1, [13, 15]),  # [demonstrating, application]
#             'slot2': idxs_to_tokens(doc1, [16, 19])  # [to, courses]
#         },
#         {
#             'arg1': idxs_to_tokens(doc1, [19]),  # [courses]
#             'pred': idxs_to_tokens(doc1, [20, 21]),  # [generated, by]
#             'arg2': idxs_to_tokens(doc1, [27])  # [models]
#         },
#     ]
#     pattern_set = RolePatternSet()
#     role_pattern_builder = RolePatternBuilder()
#     pattern_count = 0
#     for match_example in match_examples:
#         role_pattern = role_pattern_builder.build(doc1, match_example)
#         role_pattern.name = 'pattern_{}'.format(pattern_count)
#         pattern_set.add(role_pattern)
#         matches = role_pattern.match(doc1)
#         pattern_count += 1
#     matches = pattern_set.match(doc1)
#     pprint(matches)
#     assert all([m in matches for m in match_examples])