Metadata-Version: 2.1
Name: maxentropy
Version: 0.3.0
Summary: Maximum entropy and minimum divergence models in Python
Home-page: https://github.com/PythonCharmers/maxentropy.git
Author: Ed Schofield
Author-email: ed@pythoncharmers.com
License: BSD
Description: # maxentropy: Maximum entropy and minimum divergence models in Python
        
        ## Purpose
        
        This package helps you to construct a probability distribution
        (Bayesian prior) from prior information that you encode as
        generalized moment constraints.
        
        You can use it to either:
        
        1. find the flattest distribution that meets your constraints, using the
           maximum entropy principle (discrete distributions only)
        
        2. or find the "closest" model to a given prior model (in a KL divergence
           sense) that also satisfies your additional constraints.
        
        ## Background
        
        The maximum entropy principle has been shown [Cox 1982, Jaynes 2003] to be the unique consistent approach to
        constructing a discrete probability distribution from prior information that is available as "testable information".
        
        If the constraints have the form of linear moment constraints, then
        the principle gives rise to a unique probability distribution of
        **exponential form**. Most well-known probability distributions are
        special cases of maximum entropy distributions. This includes
        uniform, geometric, exponential, Pareto, normal, von Mises, Cauchy,
        and others: see
        [here](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution).
        
        ## Examples: constructing a prior subject to known constraints
        
        See the [notebooks folder](https://github.com/PythonCharmers/maxentropy/tree/master/notebooks).
        
        ### Quickstart guide
        This is a good place to start: [Loaded die example (scikit-learn estimator API)](https://github.com/PythonCharmers/maxentropy/blob/master/notebooks/Loaded%20die%20example%20-%20skmaxent.ipynb)
        
        ## History
        This package previously lived in SciPy 
        (http://scipy.org) as ``scipy.maxentropy`` from versions v0.5 to v0.10.
        It was under-maintained and removed from SciPy v0.11. It has since been
        resurrected and refactored to use the scikit-learn Estimator inteface.
        
        ## Copyright
        (c) Ed Schofield, 2003-2019
        
Keywords: maximum-entropy minimum-divergence kullback-leibler-divergence KL-divergence bayesian-inference bayes scikit-learn sklearn prior prior-distribution
Platform: UNKNOWN
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: BSD License
Classifier: Operating System :: Microsoft :: Windows
Classifier: Operating System :: POSIX
Classifier: Operating System :: Unix
Classifier: Operating System :: MacOS
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Topic :: Software Development
Classifier: Topic :: Scientific/Engineering
Requires-Python: >=3.3
Description-Content-Type: text/markdown
